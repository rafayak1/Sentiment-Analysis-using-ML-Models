{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b885ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import librosa\n",
    "import glob\n",
    "import python_speech_features as psf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import Audio\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy import signal\n",
    "from sklearn import svm\n",
    "from scipy.signal import stft, fftconvolve\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_selection import *\n",
    "import statsmodels.api as sm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a3b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to load dataset\n",
    "def clean_audio(audio , sr):\n",
    "    cutoff_freq = 4000  # Hz\n",
    "    nyquist_freq = 0.5 * sr  # Hz (half the sampling rate)\n",
    "    order = 4\n",
    "    # Calculate filter coefficients\n",
    "    coeff1, coeff2 = signal.butter(order, cutoff_freq / nyquist_freq, 'low')\n",
    "    # Apply filter to signal\n",
    "    audio = signal.filtfilt(coeff1, coeff2, audio)\n",
    "    return audio \n",
    "\n",
    "def get_labels(folder):\n",
    "    label_string = [file[-10:-7] for file in folder]\n",
    "    labels = set(label_string)\n",
    "    mapping = dict( zip(labels , range(len(labels)) ))\n",
    "    return np.array([mapping[element] for element in label_string])\n",
    "    \n",
    "\n",
    "def get_mfcc(folder , numFeatures): \n",
    "    MFCC = np.empty([len(folder)  , numFeatures])\n",
    "    for i , file in enumerate(folder):\n",
    "        audio , sr = librosa.load(file , sr = 48000)\n",
    "        audio = clean_audio(audio , sr)\n",
    "        mfcc = librosa.feature.mfcc(audio, sr=sr, n_mfcc=numFeatures)\n",
    "        MFCC[i] = np.mean(mfcc, axis=1)\n",
    "    \n",
    "    return MFCC \n",
    "    \n",
    "def get_centroids(folder):\n",
    "    SC = np.empty([len(folder)])\n",
    "    \n",
    "    for i , file in enumerate(folder):\n",
    "        audio , sr = librosa.load(file , sr = 48000)\n",
    "        audio = clean_audio(audio , sr)\n",
    "        SC[i] = np.mean(librosa.feature.spectral_centroid(audio, sr=sr))\n",
    "        \n",
    "    return SC\n",
    "\n",
    "def get_bispectrum(folder): #\n",
    "    window_size = 1024\n",
    "    hop_size = 512\n",
    "    BS = np.empty([len(folder) , 513])\n",
    "    \n",
    "    for i,file in enumerate(folder):\n",
    "        audio , sr = librosa.load(file , sr = 48000)\n",
    "        audio = clean_audio(audio , sr)\n",
    "        f, t, stft_data = stft(audio, fs=sr, window='hann', nperseg=window_size, noverlap=hop_size)\n",
    "\n",
    "        # calculate the magnitude of the STFT data\n",
    "        stft_mag = np.abs(stft_data)\n",
    "\n",
    "        # compute the bispectrum using FFT convolutions\n",
    "        stft_mag_fft = np.fft.fft(stft_mag, axis=0)\n",
    "        stft_mag_fft_squared = np.abs(stft_mag_fft)**2\n",
    "        bisp_data_fft = np.fft.ifft(stft_mag_fft_squared, axis=0)\n",
    "        bisp_data = np.real(bisp_data_fft)\n",
    "\n",
    "        # Normalize the bispectrum data\n",
    "        bisp_data /= np.max(bisp_data)\n",
    "        bisp_data = np.mean(bisp_data , axis=1) #taking average of frequencies over time\n",
    "        BS[i] = bisp_data\n",
    "        \n",
    "    return BS\n",
    "\n",
    "def get_chromagram(folder):\n",
    "    hop_len = 512\n",
    "    n = 36\n",
    "    chroma = np.empty([len(folder) , n])\n",
    "    \n",
    "    for i , file in enumerate(folder):\n",
    "        audio , sr = librosa.load(file, sr=48000)\n",
    "        audio = clean_audio(audio , sr)\n",
    "        chromagram = librosa.feature.chroma_cqt(y=audio, sr=sr, hop_length=hop_len, n_chroma=n)\n",
    "        chroma[i] = np.mean(chromagram, axis=1)\n",
    "    return chroma\n",
    "    \n",
    "def get_spectrogram(folder):\n",
    "    hop_len = 512\n",
    "    n = 36\n",
    "    spec = np.empty([len(folder) , n])\n",
    "    \n",
    "    for i , file in enumerate(folder):\n",
    "        audio , sr = librosa.load(file, sr=48000)\n",
    "        audio = clean_audio(audio , sr)\n",
    "        mel_spec = librosa.feature.melspectrogram(audio , sr=sr , n_fft = 2048 , hop_length = hop_len , n_mels = n)\n",
    "        spec[i] = np.mean(mel_spec, axis=1)\n",
    "    return spec\n",
    "\n",
    "def get_features(folder_path): #iterates over every wav file to extract features\n",
    "    n_mfcc=28\n",
    "    folder = glob.glob(folder_path)\n",
    "    mfcc = get_mfcc(folder , n_mfcc)\n",
    "    bispectrum = get_bispectrum(folder)\n",
    "    chroma = get_chromagram(folder)\n",
    "    spec = get_spectrogram(folder)\n",
    "    centroid = get_centroids(folder)\n",
    "    #load data from storage if possible\n",
    "    X = np.append(mfcc , np.append(chroma , centroid.reshape(-1,1) , axis = 1) , axis = 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e5ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "folder_path = 'SentimentAnalysisDataset/NoiseAudioWAV/*'\n",
    "numFeatures = 28\n",
    "folder = glob.glob(folder_path)\n",
    "X = get_features(folder_path)\n",
    "Y = get_labels(glob.glob(folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f759f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used these cells if you have data in hardrive\n",
    "mfcc = np.load('mfcc.npy')\n",
    "bispectrum = np.load('bispectrum.npy')\n",
    "chroma = np.load('chroma.npy')\n",
    "mel_spec = np.load('mel_spec.npy')\n",
    "centroids = np.load('centroid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "233db92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from storage if possible\n",
    "X = np.append(mfcc , np.append(chroma , centroids.reshape(-1,1) , axis = 1) , axis = 1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245911ad",
   "metadata": {},
   "source": [
    "# Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b847ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using correlation \n",
    "def get_correlation(X , Y):\n",
    "    correlation = np.empty([len(X)])\n",
    "    for i , feature in enumerate(X) :\n",
    "        correlation[i] = np.corrcoef(feature , Y)[0,1]\n",
    "    return correlation\n",
    "\n",
    "def get_features_corr(X , Y , k):#extracting features based on correlation\n",
    "    corr = get_correlation(X.T , Y)\n",
    "    corr = np.abs(corr)\n",
    "    highest_corr = np.flip(np.argsort(corr))[:k]\n",
    "    X = np.take(X , highest_corr , axis = 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f72bcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_thr = VarianceThreshold(threshold = 1)\n",
    "# extracting features based on variance\n",
    "X1 = var_thr.fit_transform(X)\n",
    "\n",
    "#extracting features with highest correlation with Y \n",
    "X2 = get_features_corr(X , Y , 30)\n",
    "\n",
    "#extracting features using recursive feature elimination\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "rfe = RFE(model, n_features_to_select=30)\n",
    "X3 = rfe.fit_transform(X, Y)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=30)\n",
    "X4 = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2c3c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = [X, X1, X2, X3, X4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edefd67",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01bc7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split \n",
    "X_train , X_test , Y_train , Y_test = train_test_split(X3 , Y , train_size = 0.75 , shuffle = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ee6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is code used to create a summary table of how different feature extracting methods have affected accuracy score\n",
    "summary_acc = np.empty([len(X_list),5])\n",
    "summary_f1 = np.empty([len(X_list) , 5])\n",
    "for i , X0 in enumerate(X_list):\n",
    "    accuracy=[]\n",
    "    f1 = []\n",
    "    \n",
    "    X_train , X_test , Y_train , Y_test = train_test_split(X0 , Y , train_size = 0.75 , shuffle = 1)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors = 30)\n",
    "    knn.fit(X_train , Y_train)\n",
    "    prediction_knn = knn.predict(X_test)\n",
    "    f1_score(Y_test , prediction_knn, average='weighted')\n",
    "    accuracy.append(accuracy_score(Y_test , prediction_knn))\n",
    "    f1.append(f1_score(Y_test , prediction_knn , average = 'macro') )\n",
    "    \n",
    "    \n",
    "    log_reg = LogisticRegression(max_iter = 1000)\n",
    "    log_reg.fit(X_train, Y_train)\n",
    "    prediction_lr = log_reg.predict(X_test)\n",
    "    accuracy.append(accuracy_score(Y_test , prediction_lr))\n",
    "    f1.append(f1_score(Y_test , prediction_lr , average = 'macro') )\n",
    "\n",
    "    \n",
    "    \n",
    "    svm_model = svm.SVC(kernel='rbf')\n",
    "    svm_model.fit(X_train, Y_train)\n",
    "    prediction_svm = svm_model.predict(X_test)\n",
    "    accuracy.append(accuracy_score(Y_test , prediction_svm))\n",
    "    f1.append(f1_score(Y_test , prediction_svm , average = 'macro') )\n",
    "\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, Y_train)\n",
    "    prediction_gnb = gnb.predict(X_test)\n",
    "    accuracy.append(accuracy_score(Y_test , prediction_gnb))\n",
    "    f1.append(f1_score(Y_test , prediction_gnb  , average = 'macro') )\n",
    "\n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    prediction_mlp = mlp.predict(X_test)\n",
    "    accuracy.append(accuracy_score(Y_test , prediction_mlp))\n",
    "    f1.append(f1_score(Y_test , prediction_mlp ,  average = 'macro'))\n",
    "\n",
    "    \n",
    "    summary_acc[i] = accuracy\n",
    "    summary_f1[i] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fb151a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41536808, 0.44760881, 0.48361096, 0.39011284, 0.42289092],\n",
       "       [0.3707684 , 0.41106932, 0.44814616, 0.33207953, 0.38420204],\n",
       "       [0.36808168, 0.42289092, 0.45351961, 0.33852767, 0.40515852],\n",
       "       [0.4013971 , 0.43578721, 0.4545943 , 0.36969371, 0.40247179],\n",
       "       [0.39387426, 0.40515852, 0.4615798 , 0.39871037, 0.41214401]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rows indicate data after various selection techniques and columns are scores for each classifier\n",
    "summary_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40e34275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4090571 , 0.43856673, 0.47698498, 0.36059484, 0.42200913],\n",
       "       [0.36742331, 0.39890981, 0.44367277, 0.31061586, 0.38298349],\n",
       "       [0.35962583, 0.4141885 , 0.44384428, 0.31145215, 0.40437018],\n",
       "       [0.38963205, 0.42576565, 0.44293021, 0.33844034, 0.39931523],\n",
       "       [0.38377167, 0.39361662, 0.45549714, 0.39248463, 0.40908559]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_f1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
